{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c7668c",
   "metadata": {},
   "source": [
    "### RoBERTa Tutorial on VPC dataset\n",
    "https://www.youtube.com/watch?v=vNKIg8rXK6w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7108a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zenta_t2ma3ok\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\Zenta_t2ma3ok\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: 4.0.0-unsupported is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import datetime\n",
    "import json\n",
    "import math\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as stats\n",
    "import researchpy as rp\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from patsy.builtins import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torchmetrics.functional.classification import auroc\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61795e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>VPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First of all, I think you should talk to your ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello Marvelous45,  \\n\\n\\n\\nI'm sorry to hear ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You have 2 options:\\n\\n\\n\\n1. You can continue...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I believe that you should confront the owner a...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi Marvelous45, I am a college student as well...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>If you are genuinely interested and passionate...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>Hi Marvelous45, \\n\\n\\n\\nThis sounds like a tou...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>Dear Marvelous45:\\n\\nI am sorry to hear about ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>If I were you, I'd be flaming mad. Talk to you...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>As of right now, I think you should not take a...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>742 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              messages  VPC\n",
       "0    First of all, I think you should talk to your ...    5\n",
       "1    Hello Marvelous45,  \\n\\n\\n\\nI'm sorry to hear ...    5\n",
       "2    You have 2 options:\\n\\n\\n\\n1. You can continue...    3\n",
       "3    I believe that you should confront the owner a...    5\n",
       "4    Hi Marvelous45, I am a college student as well...    5\n",
       "..                                                 ...  ...\n",
       "737  If you are genuinely interested and passionate...    6\n",
       "738  Hi Marvelous45, \\n\\n\\n\\nThis sounds like a tou...    5\n",
       "739  Dear Marvelous45:\\n\\nI am sorry to hear about ...    6\n",
       "740  If I were you, I'd be flaming mad. Talk to you...    3\n",
       "741  As of right now, I think you should not take a...    5\n",
       "\n",
       "[742 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = os.path.abspath('')\n",
    "\n",
    "nlp_dir = os.path.join(dir_path, \"NPL\")\n",
    "\n",
    "vpc_data = os.path.join(nlp_dir, f\"VPC.csv\")\n",
    "\n",
    "vpc_df = pd.read_csv(vpc_data)\n",
    "vpc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09749ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 NaN messages removed\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(vpc_df[vpc_df.messages.isna()])} NaN messages removed\")\n",
    "vpc_df = vpc_df[vpc_df.messages.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb4016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZENTA_~1\\AppData\\Local\\Temp/ipykernel_28552/1079842782.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vpc_df['highlevel_VPC'] = (vpc_df.VPC - 1) / 3 + 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>VPC</th>\n",
       "      <th>highlevel_VPC</th>\n",
       "      <th>LPC</th>\n",
       "      <th>MPC</th>\n",
       "      <th>HPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First of all, I think you should talk to your ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello Marvelous45,  \\n\\n\\n\\nI'm sorry to hear ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You have 2 options:\\n\\n\\n\\n1. You can continue...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I believe that you should confront the owner a...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi Marvelous45, I am a college student as well...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>If you are genuinely interested and passionate...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>Hi Marvelous45, \\n\\n\\n\\nThis sounds like a tou...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>Dear Marvelous45:\\n\\nI am sorry to hear about ...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>If I were you, I'd be flaming mad. Talk to you...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>As of right now, I think you should not take a...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>731 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              messages  VPC  highlevel_VPC  \\\n",
       "0    First of all, I think you should talk to your ...    5              2   \n",
       "1    Hello Marvelous45,  \\n\\n\\n\\nI'm sorry to hear ...    5              2   \n",
       "2    You have 2 options:\\n\\n\\n\\n1. You can continue...    3              1   \n",
       "3    I believe that you should confront the owner a...    5              2   \n",
       "4    Hi Marvelous45, I am a college student as well...    5              2   \n",
       "..                                                 ...  ...            ...   \n",
       "737  If you are genuinely interested and passionate...    6              2   \n",
       "738  Hi Marvelous45, \\n\\n\\n\\nThis sounds like a tou...    5              2   \n",
       "739  Dear Marvelous45:\\n\\nI am sorry to hear about ...    6              2   \n",
       "740  If I were you, I'd be flaming mad. Talk to you...    3              1   \n",
       "741  As of right now, I think you should not take a...    5              2   \n",
       "\n",
       "     LPC  MPC  HPC  \n",
       "0      0    1    0  \n",
       "1      0    1    0  \n",
       "2      1    0    0  \n",
       "3      0    1    0  \n",
       "4      0    1    0  \n",
       "..   ...  ...  ...  \n",
       "737    0    1    0  \n",
       "738    0    1    0  \n",
       "739    0    1    0  \n",
       "740    1    0    0  \n",
       "741    0    1    0  \n",
       "\n",
       "[731 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpc_df['highlevel_VPC'] = (vpc_df.VPC - 1) / 3 + 1\n",
    "vpc_df = vpc_df.astype({'highlevel_VPC': 'int32'}) # Consider using 3 labels: LPC, MPC, HPC\n",
    "vpc_df['LPC'] = vpc_df.highlevel_VPC == 1\n",
    "vpc_df['MPC'] = vpc_df.highlevel_VPC == 2\n",
    "vpc_df['HPC'] = vpc_df.highlevel_VPC == 3\n",
    "vpc_df = vpc_df.astype({'LPC': 'int32'})\n",
    "vpc_df = vpc_df.astype({'MPC': 'int32'})\n",
    "vpc_df = vpc_df.astype({'HPC': 'int32'})\n",
    "vpc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d325091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPC_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, tokenizer, attributes, max_token_len: int = 128, sample = 5000):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.attributes = attributes\n",
    "        self.max_token_len = max_token_len\n",
    "        self.sample = sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        messages = str(item.messages)\n",
    "        attributes = torch.FloatTensor(item[self.attributes])\n",
    "        tokens = self.tokenizer.encode_plus(messages,\n",
    "                                            add_special_tokens=True,\n",
    "                                            return_tensors='pt',\n",
    "                                            truncation=True,\n",
    "                                            padding='max_length',\n",
    "                                            max_length=self.max_token_len,\n",
    "                                            return_attention_mask = True)\n",
    "        return {'input_ids': tokens.input_ids.flatten(), 'attention_mask': tokens.attention_mask.flatten(), 'labels': attributes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa38ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LPC  MPC  HPC\n",
       "0    1    0      455\n",
       "1    0    0      236\n",
       "0    0    1       40\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = ['LPC','MPC','HPC',]\n",
    "data = ['messages']\n",
    "\n",
    "vpc_df[attributes].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b168b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPC  MPC  HPC\n",
      "0    1    0      406\n",
      "1    0    0      220\n",
      "0    0    1       31\n",
      "dtype: int64\n",
      "LPC  MPC  HPC\n",
      "0    1    0      49\n",
      "1    0    0      16\n",
      "0    0    1       9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vpc_df[data], vpc_df[attributes], train_size=0.9, random_state=2)\n",
    "vpc_train_df = X_train.join(y_train)\n",
    "vpc_test_df = X_test.join(y_test)\n",
    "print(vpc_train_df[attributes].value_counts())\n",
    "print(vpc_test_df[attributes].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5862aa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "print(len(vpc_train_df))\n",
    "print(len(vpc_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60eae2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 2193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I had a similar situation when I was looking into what I wanted to do with my degree.\\n\\n\\n\\nI started my university as a Psychology major and realized that I didn\\'t actually want to do anything related to that field of study once I graduated.  I looked into the majors that were similar to that so I wouldn\\'t lose out on all of the courses I\\'d taken and then looked further into what I wanted to do when I graduated to decide.  After careful consideration I decided to choose a degree in Communication.\\n\\n\\n\\nWhat it sounds like in your situation is that you know what you want to do.  If Philosophy is your thing then I would say you should do it.  Regardless of the job opportunities available, if you put your mind to it, anything is possible.  Plus, Philosophy teaches you how to think in alternative ways that could be helpful in a multitude of work positions.  \\n\\n\\n\\nOne possible option you could take is to double major.  It wouldn\\'t hurt you any extra to have both of the degrees under your belt.  That honestly just opens up more possibilities for you.\\n\\n\\n\\nHowever, I don\\'t feel like you should take environmental science if you feel as if you will not do well.  What might be helpful is to find something that expands your work ability and will also be a good fit for you.  What else do you like to do?  While you are looking for a career in Philosophy having that second major might be able to land you a temporary job.  I would also take some time to figure out if its just fear that is making you think you wouldn\\'t do well or if it because you are not actually skilled in sciences.  That would be the biggest deal breaker for me.  \\n\\n\\n\\nOverall, two years is not really that long in  terms of schooling.  Even if you pursued a career in Philosophy, I feel as if you would still be taking more schooling afterwards.  Getting a job in that field requires you to have advanced to a Masters if not a Doctorate.  \\n\\n\\n\\nI feel like double majoring would be the best option for you.  You said it yourself, you are a hard worker.  You can use that drive to push through the two years and maybe even land a career in a field that emphasizes both skills.  <img src=\"../images/favicons/0.gif\" />  \\n\\n\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Max length: {vpc_df.messages.str.len().max()}\")\n",
    "lengths = vpc_df[\"messages\"].str.len()\n",
    "argmax = np.where(lengths == lengths.max())[0]\n",
    "vpc_df[\"messages\"].iloc[argmax].to_numpy().ravel().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d246719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "vpc_train_ds = VPC_Dataset(vpc_train_df, tokenizer, attributes=attributes, max_token_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d36a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([512]), torch.Size([512]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpc_train_ds.__getitem__(0)['labels'].shape, vpc_train_ds.__getitem__(0)['input_ids'].shape, vpc_train_ds.__getitem__(0)['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c0932f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,  9502,   160,   939,    74,  3608,    47,     7, 39086,   110,\n",
       "           499,   624,   209,    80,  1735,    47,    33,   156,    13,  2512,\n",
       "             4,   114, 10561,    10, 10668,  9683,    47,    32,  2778,  2509,\n",
       "            11,     8,    47, 33976,  1508,     5,  1690, 15106,  1527,     7,\n",
       "           465,    10,   633,     6,  3046,   817,    47,  1372,     4,   959,\n",
       "             6,   114,   418,     8,   110,   157,   145,    16,    55,   505,\n",
       "             7,    47,    25,   157,    25, 11098,  2992,    47,   538,    88,\n",
       "          3039,  2866,     4,    47,   189,   192,    24,    25,    22,  2678,\n",
       "           117,    80,    55,   107,   113,    53,    24,    40,    28,   966,\n",
       "            24,    11,     5,   251,   422,     4,    80,   107,    40,  3598,\n",
       "           375,    47,     8,  1010,    47,    40,  5318,     8,   386,   110,\n",
       "           301,     4, 33976,  6187,    88,   301,    98,  1335,     7,  1086,\n",
       "          3508,     9,  9499,    10,   724,    16,     5,  3251,    89,     4,\n",
       "           205, 20540,     6,  1437,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([1., 0., 0.])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpc_train_ds.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19951195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPC_Data_Module(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_data, test_data, attributes, batch_size: int = 16, max_token_length: int = 128,  model_name='roberta-base'):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.attributes = attributes\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_length = max_token_length\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        if stage in (None, \"fit\"):\n",
    "            self.train_dataset = VPC_Dataset(self.train_data, attributes=self.attributes, tokenizer=self.tokenizer, max_token_len=self.max_token_length)\n",
    "            self.test_dataset = VPC_Dataset(self.test_data, attributes=self.attributes, tokenizer=self.tokenizer, sample=None, max_token_len=self.max_token_length)\n",
    "        if stage == 'predict':\n",
    "            self.test_dataset = VPC_Dataset(self.test_data, attributes=self.attributes, tokenizer=self.tokenizer, sample=None, max_token_len=self.max_token_length)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size = self.batch_size, num_workers=4, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size = self.batch_size, num_workers=4, shuffle=False)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size = self.batch_size, num_workers=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f6c2917",
   "metadata": {},
   "outputs": [],
   "source": [
    "vpc_data_module = VPC_Data_Module(vpc_train_df, vpc_test_df, attributes=attributes, max_token_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b382ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vpc_data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1a70b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vpc_data_module.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a175a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPC_Comment_Classifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.pretrained_model = AutoModel.from_pretrained(config['model_name'], return_dict = True)\n",
    "        self.hidden = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
    "        self.classifier = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.config['n_labels'])\n",
    "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        self.loss_func = nn.BCEWithLogitsLoss(reduction='mean') # set up for multi label classification, consider different loss function\n",
    "        self.dropout = nn.Dropout()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # roberta layer\n",
    "        output = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = torch.mean(output.last_hidden_state, 1) # consider max pooling or simply taking the first hidden state\n",
    "        # final logits\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        pooled_output = self.hidden(pooled_output)\n",
    "        pooled_output = F.relu(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output) # output of model\n",
    "        # calculate loss\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.loss_func(logits.view(-1, self.config['n_labels']), labels.view(-1, self.config['n_labels']))\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch) # calls forward() due to lightning module\n",
    "        self.log(\"train loss \", loss, prog_bar = True, logger=True)\n",
    "        return {\"loss\":loss, \"predictions\":outputs, \"labels\": batch[\"labels\"]}\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch)\n",
    "        self.log(\"validation loss \", loss, prog_bar = True, logger=True)\n",
    "        return {\"val_loss\": loss, \"predictions\":outputs, \"labels\": batch[\"labels\"]}\n",
    "\n",
    "    def predict_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch)\n",
    "        return outputs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay']) # use torch.optim.AdamW\n",
    "        total_steps = self.config['train_size']/self.config['batch_size']\n",
    "        warmup_steps = math.floor(total_steps * self.config['warmup'])\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "        return [optimizer],[scheduler]\n",
    "\n",
    "  # def validation_epoch_end(self, outputs):\n",
    "  #   losses = []\n",
    "  #   for output in outputs:\n",
    "  #     loss = output['val_loss'].detach().cpu()\n",
    "  #     losses.append(loss)\n",
    "  #   avg_loss = torch.mean(torch.stack(losses))\n",
    "  #   self.log(\"avg_val_loss\", avg_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a9c0108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'model_name': 'distilroberta-base', # smaller model\n",
    "    'n_labels': len(attributes),\n",
    "    'batch_size': len(vpc_data_module.train_dataloader()), # consider alternatives\n",
    "    'lr': 1.5e-6,\n",
    "    'warmup': 0.2, \n",
    "    'train_size': len(vpc_data_module.train_dataloader()),\n",
    "    'weight_decay': 0.001,\n",
    "    'n_epochs': 100\n",
    "}\n",
    "\n",
    "model = VPC_Comment_Classifier(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee35e492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([1, 3]) tensor([[-0.3104,  0.1536,  1.0572]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "input_ids = vpc_train_ds.__getitem__(idx)['input_ids']\n",
    "\n",
    "attention_mask = vpc_train_ds.__getitem__(idx)['attention_mask']\n",
    "labels = vpc_train_ds.__getitem__(idx)['labels']\n",
    "model.cpu()\n",
    "loss, output = model(input_ids.unsqueeze(dim=0), attention_mask.unsqueeze(dim=0), labels.unsqueeze(dim=0))\n",
    "print(labels.shape, output.shape, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17d9fc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce75b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datamodule\n",
    "vpc_data_module = VPC_Data_Module(vpc_train_df, vpc_test_df, attributes=attributes, max_token_length=512, batch_size=config['batch_size'])\n",
    "vpc_data_module.setup()\n",
    "\n",
    "# model\n",
    "model = VPC_Comment_Classifier(config)\n",
    "\n",
    "# trainer and fit\n",
    "trainer = pl.Trainer(max_epochs=config['n_epochs'], num_sanity_val_steps=10)\n",
    "trainer.fit(model, vpc_data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
